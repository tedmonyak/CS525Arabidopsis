{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ae06315-6765-401e-ac3b-4868ea7b3840",
   "metadata": {},
   "source": [
    "Data: https://zenodo.org/records/10946767\n",
    "\n",
    "The goal of the project is to create a deep learning network to predict DNA accessibility across multiple Arabidopsis experiments. The data is available on zenodo. It contains both raw read coverage files (aka BigWig files) and peak files in BED format, like the files you used in Assignment 3.  For the project, consider the problem as a regression problem, i.e. your task is to predict predict read coverage rather than the presence of peaks.\n",
    "\n",
    "The zenodo repository also includes a metadata spreadsheet that indicates the source of the biological samples (the project ID and Accession number columns) as well as the plant tissue that was used to generate the samples.\n",
    "\n",
    "In training and evaluating your models, we suggest you use chromosomes 1-4 for training and validation and chromosome 5 for testing.\n",
    "\n",
    "In your project we expect you to evaluate different architectures (e.g. purely convolutional vs transformer), explore them in terms of depth and other aspects of the design (e.g. regulation and other features such as layer normalization), and perform an analysis of the filters learned by the network.  The objective is for you to develop some intuition of what works or doesn't work in this domain.\n",
    "\n",
    "In designing your approach we recommend carefully studying the approach used in the Basenji paper that will be discussed in class.  The following paper is another useful resource:\n",
    "\n",
    "Toneyan, S., Tang, Z. & Koo, P.K. Evaluating deep learning for predicting epigenomic profiles. Nature Machine Intelligence 4, 1088â€“1100 (2022).  https://doi.org/10.1038/s42256-022-00570-9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee4da94",
   "metadata": {},
   "source": [
    "Goal\n",
    "Predict DNA accessibility sites across different Arabidopsis experiments.\n",
    "\n",
    "Problem Framing:\n",
    "Given input DNA sequence, predict read coverage as a continuous, quantitative response variable.\n",
    "\n",
    "Data\n",
    "Raw Data\n",
    "Raw read coverage local filepaths (similar to those from Bassenji)\n",
    "Local filepaths of (BED) peaks (like those from Assignment 3)\n",
    "Arabidopsis genome\n",
    "\n",
    "Metadata\n",
    "Source of biological samples (project ID and Accession)\n",
    "Plant tissue identifier\n",
    "\n",
    "Training Data\n",
    "Chromosomes 1-4 will be randomly split 80/20 into train and validation data. Chromosome 5 will be held out as a test set.\n",
    "\n",
    "Data Loading\n",
    "Load in the multiple Arabidopsis experiments read coverage data, and Arabidopsis genome\n",
    "Generate testing, training, and validation set generators\n",
    "Use generator objects that will get the one hot encoded training, testing, and validation datasets so that the datasets can be randomized for each run\n",
    "\n",
    "Output: List of sequences, and coverage map of those sequences\n",
    "\n",
    "Biological Datasets\n",
    "Biologically relevant parts of the genome will be curated into datasets to explore how the models perform with known biological functions using annotations. All of these annotations will hopefully be available on ENCODE or other online resources.\n",
    "\n",
    "- Promoter dataset\n",
    "- Enhancer dataset\n",
    "- CTCF dataset \n",
    "\n",
    "\n",
    "Architecture\n",
    "We are proposing 3 different model architectures based on what we have discussed in class:\n",
    "\n",
    "- Basset model\n",
    "Small input sequence length\n",
    "3 convolutional filters\n",
    "\n",
    "- Bassenji model\n",
    "Large input sequence length (10s of kb)\n",
    "4 convolutional filters + 5 dilated convolutional filters (Arabidopsis has a ~10x smaller genome than humans)\n",
    "\n",
    "- Bassenji model with transformers\n",
    "Use positional encoding + multi-head attention layer\n",
    "\n",
    "Hyperparameters\n",
    "There are various hyperparameters that we aim to experiment with. Since we are predicting a continuous variable with a regression function, we will use a Poisson regression loss function, as done in the Bassenji model. We may look into GPyOpt (https://github.com/SheffieldML/GPyOpt) for hyperparameter optimization, but will likely just experiment with the hyperparameters manually.\n",
    "Hyper Parameters to Test (not all hyper parameters apply to all networks):\n",
    "Learning rate\n",
    "Number of layers\n",
    "Batch size\n",
    "Convolutional filter size\n",
    "Number of convolutional filters\n",
    "Input dropout rate (to inform performance on noisy data)\n",
    "Dropout rate\n",
    "Num. attention heads\n",
    "Input layer size\n",
    "Read length\n",
    "\n",
    "Prediction\n",
    "Our prediction is that the Bassenji model will be the best performing, followed by the basic Bassenji, followed by the Basset. Since we have already implemented something similar to the Basset model, this will serve as a useful benchmark.\n",
    "\n",
    "Biological Interpretation\n",
    "We aim to provide a rigorous interpretation of the biological significance of our model results, taking into account performance across different cell types, optimal read length, optimal convolutional filter size and number of filters, and other relevant aspects of model architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e196eb-0c82-45c7-ab76-e59bb4c2331c",
   "metadata": {},
   "source": [
    "Goal:\n",
    "Predict DNA accessibility sites across different Arabidopsis experiments.\n",
    "\n",
    "Problem Framing:\n",
    "Given input DNA sequence, predict read coverage as a continuous, quantitative response variable.\n",
    "\n",
    "Data:\n",
    "\n",
    "Raw Data\n",
    "\n",
    "Raw read coverage local filepaths (similar to those from Bassenji)\n",
    "Local filepaths of (BED) peaks (like those from Assignment 3)\n",
    "Arabidopsis genome\n",
    "\n",
    "Metadata\n",
    "\n",
    "Source of biological samples (project ID and Accession)\n",
    "Plant tissue identifier\n",
    "\n",
    "Training Data\n",
    "\n",
    "Chromosomes 1-4 will be randomly split 80/20 into train and validation data. Chromosome 5 will be held out as a test set.\n",
    "\n",
    "Data Loading\n",
    "\n",
    "Load in the multiple Arabidopsis experiments read coverage data, and Arabidopsis genome\n",
    "Generate testing, training, and validation set generators\n",
    "Use generator objects that will get the one hot encoded training, testing, and validation datasets so that the datasets can be randomized for each run\n",
    "\n",
    "Output: List of sequences, and coverage map of those sequences\n",
    "\n",
    "Biological Datasets\n",
    "\n",
    "Biologically relevant parts of the genome will be curated into datasets to explore how the models perform with known biological functions using annotations. All of these annotations will hopefully be available on ENCODE or other online resources.\n",
    "\n",
    "Promoter dataset\n",
    "\n",
    "Enhancer dataset\n",
    "\n",
    "CTCF dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd35fad2-8199-4eeb-9dcf-bae518081244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import gzip\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy.signal\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch,torchvision\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "torch.manual_seed(42);\n",
    "\n",
    "from Bio import SeqIO\n",
    "\n",
    "device = (\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "#device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd44ebc0-545a-4800-8bae-a6f219d1755c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (416453781.py, line 39)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[18], line 39\u001b[0;36m\u001b[0m\n\u001b[0;31m    # TODO: implement\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "def generate_fasta(bed_fname, output_train_fname, output_test_fname, seq_len):\n",
    "    bed_peaks = {'Chr1': [],\n",
    "                 'Chr2': [],\n",
    "                 'Chr3': [],\n",
    "                 'Chr4': [],\n",
    "                 'Chr5': []}\n",
    "    with gzip.open(bed_fname, \"rt\") as handle:\n",
    "        for line in handle:\n",
    "            L = line.strip().split()\n",
    "            chr_id = L[0]\n",
    "            chr_start = int(L[1])\n",
    "            chr_end = int(L[2])\n",
    "            bed_peaks[chr_id].append(math.floor((chr_start + chr_end)/2))\n",
    "\n",
    "    seqs = {}\n",
    "    output_train_file = open(output_train_fname, \"w\")\n",
    "    output_test_file = open(output_test_fname, \"w\")\n",
    "    for chr_id, chr_fname in chr_fnames.items():\n",
    "        with gzip.open(chr_fname, \"rt\") as handle:\n",
    "            for record in SeqIO.parse(handle, \"fasta\"):\n",
    "                chr_seq = str(record.seq)\n",
    "                peaks = bed_peaks[chr_id]\n",
    "                for p in peaks:\n",
    "                    start = p - int(seq_len/2)\n",
    "                    end = p + int(seq_len/2)\n",
    "                    if start < 0:\n",
    "                        start = 0\n",
    "                        end = seq_len\n",
    "                    key = \",\".join([chr_id, str(start), str(end), str(end-start)])\n",
    "                    seqs[key] = chr_seq[start:end]\n",
    "    for seq_id, seq in seqs.items():\n",
    "        chr_id = seq_id[0:4]\n",
    "        # Chromosomes 1-4 will be used for training and validation\n",
    "        if chr_id == 'Chr5':\n",
    "            output_test_file.write(\">\" + seq_id + \"\\n\" + seq + \"\\n\")\n",
    "        else:\n",
    "            output_train_file.write(\">\" + seq_id + \"\\n\" + seq + \"\\n\")\n",
    "\n",
    "# TODO: implement\n",
    "def read_bigwig(bw_fname):\n",
    "    print(bw_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adb0cf22-cedd-4cb0-b82d-3486bd827760",
   "metadata": {},
   "outputs": [],
   "source": [
    "chr_fnames = {'Chr1': '../project/Arabidopsis_thaliana.TAIR10.dna.chromosome.1.fa.gz',\n",
    "              'Chr2': '../project/Arabidopsis_thaliana.TAIR10.dna.chromosome.2.fa.gz',\n",
    "              'Chr3': '../project/Arabidopsis_thaliana.TAIR10.dna.chromosome.3.fa.gz',\n",
    "              'Chr4': '../project/Arabidopsis_thaliana.TAIR10.dna.chromosome.4.fa.gz',\n",
    "              'Chr5': '../project/Arabidopsis_thaliana.TAIR10.dna.chromosome.5.fa.gz'}\n",
    "\n",
    "#bed_fname = '../project/chromatin_cs425/SRP034156/SRX391990.target.all.bed.gz'\n",
    "\n",
    "#generate_fasta(bed_fname,\n",
    "#               '../project/chromatin_cs425/SRP034156/fasta/train_val.fasta',\n",
    "#               '../project/chromatin_cs425/SRP034156/fasta/test.fasta',\n",
    "#               1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b17a5c-7eb9-47f0-9326-73047419a28f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
