{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ae06315-6765-401e-ac3b-4868ea7b3840",
   "metadata": {},
   "source": [
    "Data: https://zenodo.org/records/10946767\n",
    "\n",
    "The goal of the project is to create a deep learning network to predict DNA accessibility across multiple Arabidopsis experiments. The data is available on zenodo. It contains both raw read coverage files (aka BigWig files) and peak files in BED format, like the files you used in Assignment 3.  For the project, consider the problem as a regression problem, i.e. your task is to predict predict read coverage rather than the presence of peaks.\n",
    "\n",
    "The zenodo repository also includes a metadata spreadsheet that indicates the source of the biological samples (the project ID and Accession number columns) as well as the plant tissue that was used to generate the samples.\n",
    "\n",
    "In training and evaluating your models, we suggest you use chromosomes 1-4 for training and validation and chromosome 5 for testing.\n",
    "\n",
    "In your project we expect you to evaluate different architectures (e.g. purely convolutional vs transformer), explore them in terms of depth and other aspects of the design (e.g. regulation and other features such as layer normalization), and perform an analysis of the filters learned by the network.  The objective is for you to develop some intuition of what works or doesn't work in this domain.\n",
    "\n",
    "In designing your approach we recommend carefully studying the approach used in the Basenji paper that will be discussed in class.  The following paper is another useful resource:\n",
    "\n",
    "Toneyan, S., Tang, Z. & Koo, P.K. Evaluating deep learning for predicting epigenomic profiles. Nature Machine Intelligence 4, 1088â€“1100 (2022).  https://doi.org/10.1038/s42256-022-00570-9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee4da94",
   "metadata": {},
   "source": [
    "Goal\n",
    "Predict DNA accessibility sites across different Arabidopsis experiments.\n",
    "\n",
    "Problem Framing:\n",
    "Given input DNA sequence, predict read coverage as a continuous, quantitative response variable.\n",
    "\n",
    "Data\n",
    "Raw Data\n",
    "Raw read coverage local filepaths (similar to those from Bassenji)\n",
    "Local filepaths of (BED) peaks (like those from Assignment 3)\n",
    "Arabidopsis genome\n",
    "\n",
    "Metadata\n",
    "Source of biological samples (project ID and Accession)\n",
    "Plant tissue identifier\n",
    "\n",
    "Training Data\n",
    "Chromosomes 1-4 will be randomly split 80/20 into train and validation data. Chromosome 5 will be held out as a test set.\n",
    "\n",
    "Data Loading\n",
    "Load in the multiple Arabidopsis experiments read coverage data, and Arabidopsis genome\n",
    "Generate testing, training, and validation set generators\n",
    "Use generator objects that will get the one hot encoded training, testing, and validation datasets so that the datasets can be randomized for each run\n",
    "\n",
    "Output: List of sequences, and coverage map of those sequences\n",
    "\n",
    "Biological Datasets\n",
    "Biologically relevant parts of the genome will be curated into datasets to explore how the models perform with known biological functions using annotations. All of these annotations will hopefully be available on ENCODE or other online resources.\n",
    "\n",
    "- Promoter dataset\n",
    "- Enhancer dataset\n",
    "- CTCF dataset \n",
    "\n",
    "\n",
    "Architecture\n",
    "We are proposing 3 different model architectures based on what we have discussed in class:\n",
    "\n",
    "- Basset model\n",
    "Small input sequence length\n",
    "3 convolutional filters\n",
    "\n",
    "- Bassenji model\n",
    "Large input sequence length (10s of kb)\n",
    "4 convolutional filters + 5 dilated convolutional filters (Arabidopsis has a ~10x smaller genome than humans)\n",
    "\n",
    "- Bassenji model with transformers\n",
    "Use positional encoding + multi-head attention layer\n",
    "\n",
    "Hyperparameters\n",
    "There are various hyperparameters that we aim to experiment with. Since we are predicting a continuous variable with a regression function, we will use a Poisson regression loss function, as done in the Bassenji model. We may look into GPyOpt (https://github.com/SheffieldML/GPyOpt) for hyperparameter optimization, but will likely just experiment with the hyperparameters manually.\n",
    "Hyper Parameters to Test (not all hyper parameters apply to all networks):\n",
    "Learning rate\n",
    "Number of layers\n",
    "Batch size\n",
    "Convolutional filter size\n",
    "Number of convolutional filters\n",
    "Input dropout rate (to inform performance on noisy data)\n",
    "Dropout rate\n",
    "Num. attention heads\n",
    "Input layer size\n",
    "Read length\n",
    "\n",
    "Prediction\n",
    "Our prediction is that the Bassenji model will be the best performing, followed by the basic Bassenji, followed by the Basset. Since we have already implemented something similar to the Basset model, this will serve as a useful benchmark.\n",
    "\n",
    "Biological Interpretation\n",
    "We aim to provide a rigorous interpretation of the biological significance of our model results, taking into account performance across different cell types, optimal read length, optimal convolutional filter size and number of filters, and other relevant aspects of model architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e196eb-0c82-45c7-ab76-e59bb4c2331c",
   "metadata": {},
   "source": [
    "Goal:\n",
    "Predict DNA accessibility sites across different Arabidopsis experiments.\n",
    "\n",
    "Problem Framing:\n",
    "Given input DNA sequence, predict read coverage as a continuous, quantitative response variable.\n",
    "\n",
    "Data:\n",
    "\n",
    "Raw Data\n",
    "\n",
    "Raw read coverage local filepaths (similar to those from Bassenji)\n",
    "Local filepaths of (BED) peaks (like those from Assignment 3)\n",
    "Arabidopsis genome\n",
    "\n",
    "Metadata\n",
    "\n",
    "Source of biological samples (project ID and Accession)\n",
    "Plant tissue identifier\n",
    "\n",
    "Training Data\n",
    "\n",
    "Chromosomes 1-4 will be randomly split 80/20 into train and validation data. Chromosome 5 will be held out as a test set.\n",
    "\n",
    "Data Loading\n",
    "\n",
    "Load in the multiple Arabidopsis experiments read coverage data, and Arabidopsis genome\n",
    "Generate testing, training, and validation set generators\n",
    "Use generator objects that will get the one hot encoded training, testing, and validation datasets so that the datasets can be randomized for each run\n",
    "\n",
    "Output: List of sequences, and coverage map of those sequences\n",
    "\n",
    "Biological Datasets\n",
    "\n",
    "Biologically relevant parts of the genome will be curated into datasets to explore how the models perform with known biological functions using annotations. All of these annotations will hopefully be available on ENCODE or other online resources.\n",
    "\n",
    "Promoter dataset\n",
    "\n",
    "Enhancer dataset\n",
    "\n",
    "CTCF dataset \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b5577c-9ebe-407a-b7bb-8da4e1d139ee",
   "metadata": {},
   "source": [
    "Human introns: 1500bp\n",
    "Arabidopsis: 150bp\n",
    "\n",
    "Basset: 600bp\n",
    "Bassenji: 130kb\n",
    "Window length: 2.5kb\n",
    "\n",
    "Human genome: 3B bases\n",
    "Arabidopsis: 100mb\n",
    "\n",
    "36 outputs (for each bigwig file)\n",
    "\n",
    "Predict single value for each 2.5kb segment\n",
    "\n",
    "Since peaks can be very high\n",
    "\n",
    "Each label is a 36 dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd35fad2-8199-4eeb-9dcf-bae518081244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import glob\n",
    "import gzip\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyBigWig\n",
    "import random\n",
    "import scipy.signal\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch,torchvision\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "torch.manual_seed(42);\n",
    "\n",
    "from Bio import SeqIO\n",
    "\n",
    "device = (\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "#device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd44ebc0-545a-4800-8bae-a6f219d1755c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fasta file from the bigwig file\n",
    "# Generates sequences of length bin_size, sliding a window of size interval across\n",
    "# each chromosome\n",
    "# Assumes that chr_fnames already exists\n",
    "def generate_input_files_from_bw(bw_fname,\n",
    "                                 output_fasta,\n",
    "                                 output_faste,\n",
    "                                 seq_length=2500,\n",
    "                                 interval=1250):\n",
    "    bw = pyBigWig.open(bw_fname)\n",
    "    chrs = ['Chr1', 'Chr2', 'Chr3', 'Chr4', 'Chr5']\n",
    "    output_fasta = open(output_fasta, \"w\")\n",
    "    output_faste = open(output_faste, \"w\")\n",
    "    for chr_id in chrs:\n",
    "        chr_fname = chr_fnames[chr_id]\n",
    "        with gzip.open(chr_fname, \"rt\") as handle:\n",
    "            for record in SeqIO.parse(handle, \"fasta\"):\n",
    "                chr_seq = str(record.seq)\n",
    "                chr_len = bw.chroms(chr_id)\n",
    "                bw_idx = 0\n",
    "                while bw_idx + seq_length < chr_len:\n",
    "                    coverage = \",\".join(map(str, bw.values(chr_id, bw_idx, bw_idx + seq_length)))\n",
    "                    seq = chr_seq[bw_idx:bw_idx + seq_length]\n",
    "                    seq_id =  \",\".join([chr_id, str(bw_idx), str(bw_idx+seq_length)])\n",
    "                    output_fasta.write(\">\" + seq_id + \"\\n\" + seq + \"\\n\")\n",
    "                    output_faste.write(\">\" + seq_id + \"\\n\" + coverage + \"\\n\")\n",
    "                    bw_idx += interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adb0cf22-cedd-4cb0-b82d-3486bd827760",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '../project/chromatin_cs425'\n",
    "chr_fnames = {'Chr1': '../project/Arabidopsis_thaliana.TAIR10.dna.chromosome.1.fa.gz',\n",
    "              'Chr2': '../project/Arabidopsis_thaliana.TAIR10.dna.chromosome.2.fa.gz',\n",
    "              'Chr3': '../project/Arabidopsis_thaliana.TAIR10.dna.chromosome.3.fa.gz',\n",
    "              'Chr4': '../project/Arabidopsis_thaliana.TAIR10.dna.chromosome.4.fa.gz',\n",
    "              'Chr5': '../project/Arabidopsis_thaliana.TAIR10.dna.chromosome.5.fa.gz'}\n",
    "\n",
    "input_dirs = [os.path.join(base_dir, 'SRP034156'), os.path.join(base_dir, 'SRP300093')]\n",
    "\n",
    "bw_fname = os.path.join(base_dir, 'SRP034156', 'SRX1096548_Rep0.rpgc.bw')\n",
    "\n",
    "generate_input_files_from_bw(bw_fname,\n",
    "                             '../project/chromatin_cs425/SRP034156/fasta/SRP034156.fasta',\n",
    "                             '../project/chromatin_cs425/SRP034156/fasta/SRP034156.faste')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "045a65ea-3db2-43b7-a7e4-5a9640ba1174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leaf\n",
      "['../project/chromatin_cs425/SRP300093/SRX9770773_Rep0.rpgc.bw', '../project/chromatin_cs425/SRP300093/SRX9770774_Rep0.rpgc.bw', '../project/chromatin_cs425/SRP300093/SRX9770775_Rep0.rpgc.bw', '../project/chromatin_cs425/SRP300093/SRX9770776_Rep0.rpgc.bw', '../project/chromatin_cs425/SRP300093/SRX9770777_Rep0.rpgc.bw', '../project/chromatin_cs425/SRP300093/SRX9770778_Rep0.rpgc.bw', '../project/chromatin_cs425/SRP300093/SRX9770779_Rep0.rpgc.bw', '../project/chromatin_cs425/SRP300093/SRX9770780_Rep0.rpgc.bw', '../project/chromatin_cs425/SRP300093/SRX9770781_Rep0.rpgc.bw', '../project/chromatin_cs425/SRP300093/SRX9770782_Rep0.rpgc.bw', '../project/chromatin_cs425/SRP300093/SRX9770783_Rep0.rpgc.bw', '../project/chromatin_cs425/SRP300093/SRX9770784_Rep0.rpgc.bw', '../project/chromatin_cs425/SRP300093/SRX9770785_Rep0.rpgc.bw', '../project/chromatin_cs425/SRP300093/SRX9770786_Rep0.rpgc.bw', '../project/chromatin_cs425/SRP300093/SRX9770787_Rep0.rpgc.bw', '../project/chromatin_cs425/SRP300093/SRX9770788_Rep0.rpgc.bw', '../project/chromatin_cs425/SRP300093/SRX9770789_Rep0.rpgc.bw', '../project/chromatin_cs425/SRP300093/SRX9770790_Rep0.rpgc.bw', '../project/chromatin_cs425/SRP300093/SRX9770791_Rep0.rpgc.bw', '../project/chromatin_cs425/SRP300093/SRX9770792_Rep0.rpgc.bw', '../project/chromatin_cs425/SRP300093/SRX9770793_Rep0.rpgc.bw']\n",
      "Root\n",
      "['../project/chromatin_cs425/SRP034156/SRX1096548_Rep0.rpgc.bw', '../project/chromatin_cs425/SRP034156/SRX1096549_Rep0.rpgc.bw', '../project/chromatin_cs425/SRP034156/SRX1096550_Rep0.rpgc.bw', '../project/chromatin_cs425/SRP034156/SRX1096551_Rep0.rpgc.bw', '../project/chromatin_cs425/SRP034156/SRX1098137_Rep0.rpgc.bw', '../project/chromatin_cs425/SRP034156/SRX1098138_Rep0.rpgc.bw', '../project/chromatin_cs425/SRP034156/SRX391990_Rep0.rpgc.bw', '../project/chromatin_cs425/SRP034156/SRX391991_Rep0.rpgc.bw', '../project/chromatin_cs425/SRP034156/SRX391992_Rep0.rpgc.bw']\n",
      "Seed\n",
      "['../project/chromatin_cs425/SRP034156/SRX1098135_Rep0.rpgc.bw', '../project/chromatin_cs425/SRP034156/SRX1098136_Rep0.rpgc.bw', '../project/chromatin_cs425/SRP034156/SRX391993_Rep0.rpgc.bw', '../project/chromatin_cs425/SRP034156/SRX391994_Rep0.rpgc.bw', '../project/chromatin_cs425/SRP034156/SRX391995_Rep0.rpgc.bw']\n",
      "Flower\n",
      "['../project/chromatin_cs425/SRP034156/SRX391996_Rep0.rpgc.bw', '../project/chromatin_cs425/SRP034156/SRX391997_Rep0.rpgc.bw']\n"
     ]
    }
   ],
   "source": [
    "# Work in progress: create tissue-level fasta files by aggregating data from bigwig files\n",
    "# Uses Metadata.csv to determine identification of each bigwig file\n",
    "\n",
    "metadata = pd.read_csv(os.path.join(base_dir, 'Metadata.csv'))\n",
    "tissues = metadata['Tissue'].unique()\n",
    "for tissue in tissues:\n",
    "    t_fnames = []\n",
    "    t_metadata = metadata[metadata['Tissue'] == tissue]\n",
    "    output_train_file = os.path.join(base_dir, 'input_data', tissue, \"train_val.fasta\")\n",
    "    output_test_file = os.path.join(base_dir, 'input_data', tissue, \"test.fasta\")\n",
    "    for index, row in t_metadata.iterrows():\n",
    "        project = row.iloc[0]\n",
    "        accession = row.iloc[1]\n",
    "        fname = accession + \"_Rep0.rpgc.bw\"\n",
    "        fname = os.path.join(base_dir, project, fname)\n",
    "        t_fnames.append(fname)\n",
    "    # TODO: \n",
    "    print(tissue)\n",
    "    print(t_fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55de0c77-8a2d-44d1-8749-0b4be361b6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets can be obtained e.g. from:\n",
    "# https://github.com/MedChaabane/deepRAM/tree/master/datasets/ChIP-seq\n",
    "\n",
    "# convert sequence to a one-hot encoding\n",
    "# and pad with a uniform distribution\n",
    "def seqtopad(sequence, motif_len):\n",
    "    rows=len(sequence)+2*motif_len-2\n",
    "    S=np.empty([rows,4])\n",
    "    base=['A', 'C', 'G', 'T']\n",
    "    for i in range(rows):\n",
    "        for j in range(4):\n",
    "            if (i-motif_len+1<len(sequence) and sequence[i-motif_len+1]=='N' \n",
    "                or i<motif_len-1 or i>len(sequence)+motif_len-2):\n",
    "                S[i,j]=np.float32(0.25)\n",
    "            elif sequence[i-motif_len+1]==base[j]:\n",
    "                S[i,j]=np.float32(1)\n",
    "            else:\n",
    "                S[i,j]=np.float32(0)\n",
    "    return np.transpose(S)\n",
    "\n",
    "# TODO: determine appropriate motif_len\n",
    "def load_file(path, motif_len=24):\n",
    "    dataset=[]\n",
    "    sequences=[]\n",
    "    with open(path, \"rt\") as handle:\n",
    "        for record in SeqIO.parse(handle, \"fasta\"):\n",
    "            coverage = float((record.id).split(',')[3])\n",
    "            sequence = str(record.seq)\n",
    "            dataset.append([seqtopad(sequence, motif_len),[coverage]])\n",
    "            sequences.append(sequence)\n",
    "    return dataset  \n",
    "\n",
    "class chromatin_dataset(Dataset):\n",
    "    def __init__(self, xy):\n",
    "        self.x_data=np.array([el[0] for el in xy],dtype=np.float32)\n",
    "        self.y_data =np.array([el[1] for el in xy ],dtype=np.float32)\n",
    "        self.x_data = torch.from_numpy(self.x_data)\n",
    "        self.y_data = torch.from_numpy(self.y_data)\n",
    "        self.length=len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "def get_train_valid_test_loader(train_fname, test_fname):\n",
    "    train_val_dataset=load_file(train_fname)\n",
    "    train_data, val_test_data = train_test_split(train_val_dataset, test_size=0.25)\n",
    "    test_data = loadfile(test_fname)\n",
    "    len(train_data),len(valid_data),len(test_data)\n",
    "    \n",
    "    train_dataset=chromatin_dataset(train_data)\n",
    "    valid_dataset=chromatin_dataset(valid_data)\n",
    "    test_dataset=chromatin_dataset(test_data)\n",
    "    \n",
    "    batch_size = 64\n",
    "    train_loader = DataLoader(dataset=train_dataset,\n",
    "                              batch_size=batch_size,shuffle=True)\n",
    "    valid_loader = DataLoader(dataset=valid_dataset,\n",
    "                              batch_size=batch_size,shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset,\n",
    "                              batch_size=batch_size,shuffle=True)\n",
    "    return train_loader, valid_loader, test_loader\n",
    "\n",
    "train_loader, valid_loader, test_loader=get_train_valid_test_loader(train_fname, test_fname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
