{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9000c9a",
   "metadata": {},
   "source": [
    "# CS525 FINAL PROJECT\n",
    "### GROUP MEMBERS: Ted Monyak, Jack Forman\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0da78238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "import logomaker\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "\n",
    "from src import get_data_loaders\n",
    "\n",
    "device = (\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "device = torch.device(device)\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f9d487",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ca0069",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "##### Training, Validation, Testing Dataset\n",
    "\n",
    "##### Biological Dataset\n",
    "\n",
    "To examine the biological relevance and generalizability of our models, we curated an additional dataset from [Ensembl Plants](https://ftp.ensemblgenomes.ebi.ac.uk/pub/plants/release-61/fasta/arabidopsis_thaliana/pep/), consisting of approximately 27,000 protein-coding genes from the *Arabidopsis thaliana* genome. The aim of this dataset is to investigate the chromatin accessibility of these genes across various plant tissues. This is biologically significant, as different tissues express distinct sets of proteins, and gene accessibility plays a key regulatory role in this expression. \n",
    "\n",
    "We expect the results to reveal two main patterns:  \n",
    "1. A core set of genes with consistent accessibility across all tissues  \n",
    "2. A set of genes exhibiting tissue-specific accessibility profiles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d52ad7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sequences from sequences.fasta\n",
      "Loading coverage from SRX391990.faste\n",
      "Loading coverage from SRX9770779.faste\n",
      "Loading coverage from SRX9770784.faste\n",
      "Loading coverage from SRX9770786.faste\n",
      "Loading coverage from SRX391992.faste\n",
      "Loading coverage from SRX391996.faste\n",
      "Loading coverage from SRX9770782.faste\n",
      "Loading coverage from SRX1098138.faste\n",
      "Loading coverage from SRX9770780.faste\n",
      "Loading coverage from SRX391994.faste\n",
      "Loading coverage from SRX9770787.faste\n",
      "Loading coverage from SRX391993.faste\n",
      "Loading coverage from SRX9770778.faste\n",
      "Loading coverage from SRX391991.faste\n",
      "Loading coverage from SRX9770785.faste\n",
      "Loading coverage from SRX9770781.faste\n",
      "Loading coverage from SRX391995.faste\n",
      "Loading coverage from SRX391997.faste\n",
      "Loading coverage from SRX9770783.faste\n",
      "Loading coverage from SRX1098137.faste\n",
      "Loading coverage from SRX1098135.faste\n",
      "Loading coverage from SRX1096550.faste\n",
      "Loading coverage from SRX9770774.faste\n",
      "Loading coverage from SRX9770789.faste\n",
      "Loading coverage from SRX9770790.faste\n",
      "Loading coverage from SRX1096549.faste\n",
      "Loading coverage from SRX9770792.faste\n",
      "Loading coverage from SRX9770776.faste\n",
      "Loading coverage from SRX9770773.faste\n",
      "Loading coverage from SRX1098136.faste\n",
      "Loading coverage from SRX9770793.faste\n",
      "Loading coverage from SRX9770777.faste\n",
      "Loading coverage from SRX9770775.faste\n",
      "Loading coverage from SRX9770788.faste\n",
      "Loading coverage from SRX1096551.faste\n",
      "Loading coverage from SRX1096548.faste\n",
      "Loading coverage from SRX9770791.faste\n",
      "Done Loading Data\n"
     ]
    }
   ],
   "source": [
    "data_dir=os.path.join(os.getcwd(), 'Data', 'Parsed_Data_Window')\n",
    "train_val_data_to_load=math.inf\n",
    "test_data_to_load=math.inf\n",
    "faste_files_to_load = 37\n",
    "normalize = False\n",
    "batch_size = 128\n",
    "train_loader, val_loader, test_loader = get_data_loaders(\n",
    "    data_dir=data_dir,\n",
    "    train_val_data_to_load=train_val_data_to_load,\n",
    "    test_data_to_load=test_data_to_load,\n",
    "    batch_size=batch_size,\n",
    "    faste_files_to_load=faste_files_to_load,\n",
    "    normalize=normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e491ba",
   "metadata": {},
   "source": [
    "### Model Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8a5bf7",
   "metadata": {},
   "source": [
    "Several neural network architectures were tested to evaluate their effectiveness in predicting chromatin accessibility. In the simplest case—the **Simple CNN**—a single convolutional layer was followed by a fully connected layer. This minimal architecture offers advantages in terms of interpretability and computational efficiency. However, its simplicity may limit its ability to capture higher-order patterns in the sequence data that could be important for accurately modeling chromatin accessibility.\n",
    "\n",
    "A more complex architecture—the **Deep CNN**—consisted of three convolutional layers followed by three fully connected layers. This model has significantly greater capacity to learn complex features and hierarchical patterns within the sequence data, while still retaining a relatively straightforward structure. Its depth allows it to capture more nuanced relationships that may be missed by simpler model.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1947b62",
   "metadata": {},
   "source": [
    "# Methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326485cf",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "281343fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalDnaCnn(nn.Module):\n",
    "    def __init__(self, num_kernels=[320, 128, 64, 32], kernel_size=[10,10,10,10],\n",
    "                 dropout=0, transformer_heads=4, transformer_layers=3):\n",
    "        super(LocalDnaCnn, self).__init__()\n",
    "        self.input_channels=4\n",
    "        self.num_kernels=num_kernels\n",
    "        self.kernel_size=kernel_size\n",
    "        self.dropout=dropout\n",
    "        self.transformer_heads=transformer_heads\n",
    "        self.transformer_layers=transformer_layers\n",
    "        self.conv_block = nn.Sequential(\n",
    "            # first layer\n",
    "            nn.Conv1d(in_channels=self.input_channels,\n",
    "                      out_channels=num_kernels[0],\n",
    "                      kernel_size=kernel_size[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "        )\n",
    "        # second layer\n",
    "        self.conv_block.append(nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.num_kernels[0],\n",
    "                      out_channels=num_kernels[1],\n",
    "                      kernel_size=kernel_size[1]),\n",
    "            #nn.BatchNorm1d(num_features=num_kernels[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout),   \n",
    "            nn.MaxPool1d(kernel_size=2),        \n",
    "        ))\n",
    "        # Add a third convolutional layer\n",
    "        self.conv_block.append(nn.Sequential(\n",
    "            # second layer\n",
    "            nn.Conv1d(in_channels=self.num_kernels[1],\n",
    "                      out_channels=num_kernels[2],\n",
    "                      kernel_size=kernel_size[2]),\n",
    "            #nn.BatchNorm1d(num_features=num_kernels[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout),   \n",
    "            nn.MaxPool1d(kernel_size=2),  \n",
    "        ))\n",
    "        # Add a fourth convolutional layer\n",
    "        self.conv_block.append(nn.Sequential(\n",
    "            # second layer\n",
    "            nn.Conv1d(in_channels=self.num_kernels[2],\n",
    "                      out_channels=num_kernels[3],\n",
    "                      kernel_size=kernel_size[3]),\n",
    "        #    #nn.BatchNorm1d(num_features=num_kernels[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout),\n",
    "            nn.MaxPool1d(kernel_size=2),  \n",
    "        ))\n",
    "        #the transformer is a combo of multiple encoder layers\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(2144,\n",
    "                                       nhead=self.transformer_heads),\n",
    "                                       self.transformer_layers)\n",
    "        self.attention_weights = None  # Store attention weights\n",
    "        self.regression_block = nn.Sequential(\n",
    "            nn.Linear(4704, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout),\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=self.dropout),\n",
    "            nn.Linear(128, faste_files_to_load),\n",
    "        ) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        x =  torch.flatten(x, 1)\n",
    "        #x,_ = torch.max(x, dim=2) \n",
    "        #x = self.transformer(x)     \n",
    "        x = self.regression_block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d93fed9",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7495920",
   "metadata": {},
   "source": [
    "### Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41fc36a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, model, loss_fn, optimizer, epoch):\n",
    "    size = len(dataloader)\n",
    "    num_batches = len(dataloader)\n",
    "    total_loss = 0\n",
    "    # set the model to training mode - important when you have \n",
    "    # batch normalization and dropout layers\n",
    "    model.train()\n",
    "    for batch_idx, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # Compute prediction and loss\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if epoch % 10 == 0 :\n",
    "        print(f\"training loss: {total_loss/num_batches:>7f}\")\n",
    "    return total_loss / num_batches\n",
    "\n",
    "def validation(dataloader, model, loss_fn, epoch):\n",
    "    # set the model to evaluation mode \n",
    "    model.eval()\n",
    "    # size of dataset\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    validation_loss = 0\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients \n",
    "    # are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage \n",
    "    # for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_pred = model(X)\n",
    "            validation_loss += loss_fn(y_pred, y).item()\n",
    "    validation_loss /= num_batches\n",
    "    if epoch%10 == 0 :\n",
    "        print(f\"Validation Loss: {validation_loss:>8f} \\n\")\n",
    "    return validation_loss\n",
    "\n",
    "def train_model(train_loader, val_loader, model, optimizer, loss_fn, lr_scheduler, epochs, overfit_ratio=0.85):\n",
    "\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    for t in range(epochs):\n",
    "        if t % 1 == 0 :\n",
    "            print(f\"Epoch {t}\\n-------------------------------\")\n",
    "        loss = train_epoch(train_loader, model, loss_fn, optimizer, t)\n",
    "        train_loss.append(loss)\n",
    "        loss = validation(val_loader, model, loss_fn, t)\n",
    "        validation_loss.append(loss)\n",
    "    \n",
    "        if train_loss[-1] < validation_loss[-1]:\n",
    "            # print(f\"Training loss {train_loss[-1]} is less than validation loss {validation_loss[-1]}\")\n",
    "\n",
    "            if train_loss[-1]/validation_loss[-1] < overfit_ratio:\n",
    "                print(f\"Training loss {train_loss[-1]} is well below validation loss {validation_loss[-1]}\")\n",
    "                break\n",
    "        lr_scheduler.step()\n",
    "                \n",
    "    print(\"Done!\")\n",
    "\n",
    "    def plot_loss(train_loss, validation_loss):\n",
    "        plt.figure(figsize=(4,3))\n",
    "        plt.plot(np.arange(len(train_loss)), train_loss, label='Training')\n",
    "        plt.plot(np.arange(len(validation_loss)), validation_loss, label='Validation')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        # plt.savefig('')\n",
    "        plt.show()\n",
    "    plot_loss(train_loss, validation_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3058e1",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a06b879",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tedmonyak/miniconda3/envs/gp/lib/python3.12/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "-------------------------------\n",
      "training loss: 0.917787\n",
      "Validation Loss: 0.920516 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch 2\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_kernels=[320, 128, 64, 32]\n",
    "kernel_size=[10,10,10,10]\n",
    "dropout=0.3\n",
    "transformer_heads=4\n",
    "transformer_layers=3\n",
    "lr = 0.001\n",
    "faste_files_to_load=37\n",
    "decay_rate = 0.97\n",
    "\n",
    "model = LocalDnaCnn(num_kernels=num_kernels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    dropout=dropout,\n",
    "                    transformer_heads=transformer_heads,\n",
    "                    transformer_layers=transformer_layers).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.PoissonNLLLoss(log_input=True, full=True)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decay_rate)\n",
    "train_model(train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            loss_fn=loss_fn,\n",
    "            lr_scheduler=lr_scheduler,\n",
    "            epochs=30,\n",
    "            overfit_ratio=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2106a45",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295db9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fname = 'Ted_Models/model.pth'\n",
    "torch.save(model, model_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e11c89",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d936d5",
   "metadata": {},
   "source": [
    "### Validate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af6a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_load = model_fname\n",
    "model = torch.load(model_to_load)\n",
    "#model.to(device)\n",
    "#model.eval()\n",
    "with torch.no_grad():\n",
    "        model.eval()\n",
    "        pred_list = []\n",
    "        labels_list = []\n",
    "        for batch_index, (X, y) in enumerate(test_loader):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model(X)\n",
    "            y_pred = torch.exp(y_pred)\n",
    "            y_pred = torch.flatten(y_pred).cpu().detach().numpy()\n",
    "            pred_list.append(y_pred)\n",
    "\n",
    "            y = torch.flatten(y).cpu().numpy()\n",
    "            labels_list.append(y)\n",
    "            \n",
    "        labels = np.concatenate(labels_list)\n",
    "        predictions = np.concatenate(pred_list)\n",
    "        \n",
    "pearson_r = np.corrcoef(labels, predictions)[0, 1]\n",
    "\n",
    "plt.scatter(labels, predictions)\n",
    "plt.xlabel(\"Experiment Coverage\")\n",
    "plt.ylabel(\"Predicted Coverage\")\n",
    "plt.title(\"Model Accuracy on Test Set (Chromosome 5)\")\n",
    "plt.text(0.1, 0.9, f\"r = {pearson_r:.2f}\", transform=plt.gca().transAxes)\n",
    "degree = 1\n",
    "coeffs = np.polyfit(labels, predictions, degree)\n",
    "polynomial = np.poly1d(coeffs)\n",
    "X_plot = np.linspace(plt.gca().get_xlim()[0], plt.gca().get_xlim()[1], 100)\n",
    "y_plot = polynomial(X_plot)\n",
    "plt.plot(X_plot, y_plot, color='red')\n",
    "plt.savefig('LocalDnaCnnAccuracy.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0869dfe9",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e4103c",
   "metadata": {},
   "source": [
    "### Simple CNN\n",
    "<style>\n",
    "img {\n",
    "    display: block;\n",
    "    margin-left: 0;\n",
    "    margin-right: auto;\n",
    "    width: 30%;\n",
    "    height: 30%;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "Discussion about training accuracy  \n",
    "![image](Model_SimpleCNN_Best/TrainingAccuracy.png)\n",
    "\n",
    "Discussion about validation accuracy  \n",
    "![image](Model_SimpleCNN_Best/ValAccuracy.png)\n",
    "\n",
    "Discussion about testing accuracy  \n",
    "![image](Model_SimpleCNN_Best/TestingAccuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7725993",
   "metadata": {},
   "source": [
    "### Deeper CCN\n",
    "<style>\n",
    "img {\n",
    "    display: block;\n",
    "    margin-left: 0;\n",
    "    margin-right: auto;\n",
    "    width: 30%;\n",
    "    height: 30%;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "Discussion about training accuracy  \n",
    "![image](Model_DeepCNN_Best/TrainingAccuracy.png)\n",
    "\n",
    "Discussion about validation accuracy  \n",
    "![image](Model_DeepCNN_Best/ValAccuracy.png)\n",
    "\n",
    "Discussion about testing accuracy  \n",
    "![image](Model_DeepCNN_Best/TestingAccuracy.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e6f001",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "<style>\n",
    "img {\n",
    "    display: block;\n",
    "    margin-left: 0;\n",
    "    margin-right: auto;\n",
    "    width: 30%;\n",
    "    height: 30%;\n",
    "}\n",
    "</style>\n",
    "Discussion about training accuracy ![image](Model_DNATranformer_Best/TrainingAccuracy.png) \n",
    "Discussion about validation accuracy ![image](Model_DNATranformer_Best/ValAccuracy.png) \n",
    "Discussion about testing accuracy ![image](Model_DNATranformer_Best/TestingAccuracy.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e63492",
   "metadata": {},
   "source": [
    "### Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25cc925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entropy(row):\n",
    "    H = 0\n",
    "    for n in ['A', 'C', 'G', 'T']:\n",
    "        H -= row[n] * math.log(row[n])\n",
    "    return H\n",
    "\n",
    "def create_profile_logo(motif_probs):\n",
    "    \"\"\"\n",
    "    Creates a profile logo based on the motif probabilities\n",
    "\n",
    "    parameters:\n",
    "    motif_probs: a 2D array, with dims [k, 4], populated with\n",
    "    the probabilities of each nucleotide at each position\n",
    "\n",
    "    Displays the profile logo\n",
    "    \"\"\"\n",
    "    mat = pd.DataFrame(motif_probs)\n",
    "    mat.columns = ['A', 'C', 'G', 'T']\n",
    "    logomaker.Logo(mat)\n",
    "\n",
    "weights=model.conv_block[0].weight.detach().cpu().numpy()\n",
    "max_h = 0\n",
    "max_i = 0\n",
    "for i in range(len(weights)) :\n",
    "    motif = pd.DataFrame(weights[i].T, columns=['A','C','G','T']).abs()\n",
    "    motif['entropy'] = motif.apply(calc_entropy, axis=1)\n",
    "    total_entropy = motif['entropy'].sum()\n",
    "    if total_entropy > max_h:\n",
    "        max_h = total_entropy\n",
    "        max_i = i\n",
    "\n",
    "motif = pd.DataFrame(weights[max_i].T, columns=['A','C','G','T'])\n",
    "motif.plot.bar(stacked=True, use_index=True)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.imshow(weights[max_i],cmap=\"Blues\")\n",
    "plt.colorbar()\n",
    "\n",
    "create_profile_logo(motif.abs())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a4b1f5",
   "metadata": {},
   "source": [
    "### Gene differnece in Tissues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5e59bd",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ef899e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
