{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "481260f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from Load_Data import load_data\n",
    "\n",
    "device = (\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "device = torch.device(device)\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b410bb37",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52960819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sequences from sequences.fasta\n",
      "Loading coverage from SRX391990.faste\n",
      "Loading coverage from SRX9770779.faste\n",
      "Loading coverage from SRX9770784.faste\n",
      "Loading coverage from SRX9770786.faste\n",
      "Loading coverage from SRX391992.faste\n",
      "Loading coverage from SRX391996.faste\n",
      "Loading coverage from SRX9770782.faste\n",
      "Loading coverage from SRX1098138.faste\n",
      "Loading coverage from SRX9770780.faste\n",
      "Loading coverage from SRX391994.faste\n",
      "Loading coverage from SRX9770787.faste\n",
      "Loading coverage from SRX391993.faste\n",
      "Loading coverage from SRX9770778.faste\n",
      "Loading coverage from SRX391991.faste\n",
      "Loading coverage from SRX9770785.faste\n"
     ]
    }
   ],
   "source": [
    "def get_data_loaders(batch_size=64):\n",
    "    Data = load_data(os.path.join(os.getcwd(), 'Data', 'Parsed_Data'), \n",
    "                        train_val_data_to_load=1000, \n",
    "                        test_data_to_load=100\n",
    "                        train_val_data_to_load=2000, \n",
    "                        test_data_to_load=1\n",
    "                        )\n",
    "    \n",
    "    training_dataset, validation_dataset, testing_dataset = Data\n",
    "\n",
    "    train_loader = DataLoader(dataset=training_dataset,\n",
    "                              batch_size=batch_size,shuffle=True)\n",
    "    val_loader = DataLoader(dataset=validation_dataset,\n",
    "                              batch_size=batch_size,shuffle=True)\n",
    "    test_loader = DataLoader(dataset=testing_dataset,\n",
    "                              batch_size=batch_size,shuffle=True)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "train_loader, val_loader, test_loader = get_data_loaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c018f0a",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_count": 86,
   "id": "fa21dde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DnaCnn(nn.Module):\n",
    "    def __init__(self, num_kernels=[512, 256, 128], kernel_size=[1028,256,128],\n",
    "                 dropout=0):\n",
    "        super(DnaCnn, self).__init__()\n",
    "        self.input_channels=4\n",
    "        self.num_kernels=num_kernels\n",
    "        self.kernel_size=kernel_size\n",
    "        self.dropout=dropout\n",
    "        self.conv_block = nn.Sequential(\n",
    "            # first layer\n",
    "            nn.Conv1d(in_channels=self.input_channels,\n",
    "                      out_channels=num_kernels[0],\n",
    "                      kernel_size=kernel_size[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "        )\n",
    "        # second layer\n",
    "        # self.conv_block.append(nn.Sequential(\n",
    "        #     nn.Conv1d(in_channels=self.num_kernels[0],\n",
    "        #               out_channels=num_kernels[1],\n",
    "        #               kernel_size=kernel_size[1]),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool1d(kernel_size=2),\n",
    "        #     nn.Dropout(p=self.dropout),            \n",
    "        # ))\n",
    "        # Add a third convolutional layer\n",
    "        # self.conv_block.append(nn.Sequential(\n",
    "        #     # second layer\n",
    "        #     nn.Conv1d(in_channels=self.num_kernels[1],\n",
    "        #               out_channels=num_kernels[2],\n",
    "        #               kernel_size=kernel_size[2]),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool1d(kernel_size=2),\n",
    "        #     nn.Dropout(p=self.dropout),            \n",
    "        # ))\n",
    "        self.regression_block = nn.Sequential(\n",
    "            nn.Linear(num_kernels[0], 37),\n",
    "            nn.ReLU(),  # ReLU ensures positive outputs\n",
    "            # nn.LogSoftmax(dim=1)  # Apply log softmax if necessary for your task\n",
    "        )  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        x,_ = torch.max(x, dim=2)        \n",
    "        x = self.regression_block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524a327e",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d9bd1a",
   "metadata": {},
   "source": [
    "### Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_count": 87,
   "id": "71beb2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, model, loss_fn, optimizer, epoch):\n",
    "    size = len(dataloader)\n",
    "    num_batches = len(dataloader)\n",
    "    total_loss = 0\n",
    "    # set the model to training mode - important when you have \n",
    "    # batch normalization and dropout layers\n",
    "    model.train()\n",
    "    for batch_idx, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # Compute prediction and loss\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if epoch % 10 == 0 :\n",
    "        print(f\"training loss: {total_loss/num_batches:>7f}\")\n",
    "    return total_loss / num_batches\n",
    "\n",
    "def validation(dataloader, model, loss_fn, epoch):\n",
    "    # set the model to evaluation mode \n",
    "    model.eval()\n",
    "    # size of dataset\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    validation_loss, correct = 0, 0\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients \n",
    "    # are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage \n",
    "    # for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_pred = model(X)\n",
    "            validation_loss += loss_fn(y_pred, y).item()\n",
    "    validation_loss /= num_batches\n",
    "    if epoch%10 == 0 :\n",
    "        print(f\"Validation Loss: {validation_loss:>8f} \\n\")\n",
    "    return validation_loss\n",
    "\n",
    "def train_model(train_loader, val_loader, model, optimizer):\n",
    "    epochs = 1000\n",
    "    loss_fn = nn.PoissonNLLLoss(log_input=True, full=True)\n",
    "    patience = math.inf\n",
    "    p = patience\n",
    "    \n",
    "    \n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    best_loss = math.inf\n",
    "    for t in range(epochs):\n",
    "        if t % 10 == 0 :\n",
    "            print(f\"Epoch {t}\\n-------------------------------\")\n",
    "        loss = train_epoch(train_loader, model, loss_fn, optimizer, t)\n",
    "        train_loss.append(loss)\n",
    "        loss = validation(val_loader, model, loss_fn, t)\n",
    "        validation_loss.append(loss)\n",
    "    \n",
    "        if loss < best_loss:\n",
    "            best_loss = loss    \n",
    "            p = patience\n",
    "        else:\n",
    "            p -= 1\n",
    "            if p == 0:\n",
    "                print(\"Early Stopping!\")\n",
    "                break    \n",
    "    print(\"Done!\")\n",
    "\n",
    "    def plot_loss(train_loss, validation_loss):\n",
    "        plt.figure(figsize=(4,3))\n",
    "        plt.plot(np.arange(len(train_loss)), train_loss, label='Training')\n",
    "        plt.plot(np.arange(len(validation_loss)), validation_loss, label='Validation')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    plot_loss(train_loss, validation_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4196cdfa",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_count": 89,
   "id": "6f6b9d5d",
   "metadata": {},
   "outputs": [],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "-------------------------------\n",
      "training loss: 23923.468040\n",
      "Validation Loss: 23127.523047 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "training loss: 17688.430309\n",
      "Validation Loss: 16815.430859 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "training loss: 9894.751953\n",
      "Validation Loss: 9261.502246 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "training loss: 6178.538841\n",
      "Validation Loss: 5938.173047 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "training loss: 6084.803356\n",
      "Validation Loss: 5859.529785 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "training loss: 6031.431729\n",
      "Validation Loss: 5864.342578 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "training loss: 5963.293324\n",
      "Validation Loss: 5834.663184 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "training loss: 5718.509455\n",
      "Validation Loss: 5653.118604 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "training loss: 4149.658802\n",
      "Validation Loss: 4095.127100 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "training loss: 3920.775746\n",
      "Validation Loss: 4028.951807 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "training loss: 3836.891602\n",
      "Validation Loss: 4061.662305 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "training loss: 3753.528076\n",
      "Validation Loss: 3999.315234 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "training loss: 3693.816717\n",
      "Validation Loss: 3935.821875 \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "training loss: 3624.455633\n",
      "Validation Loss: 3947.173730 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "training loss: 3564.066162\n",
      "Validation Loss: 3999.240381 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "training loss: 3503.697776\n",
      "Validation Loss: 3948.585449 \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "training loss: 3459.947066\n",
      "Validation Loss: 3909.540332 \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "training loss: 3422.459939\n",
      "Validation Loss: 3949.994385 \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "training loss: 3389.212957\n",
      "Validation Loss: 3901.482959 \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "training loss: 3354.320512\n",
      "Validation Loss: 3864.371875 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "training loss: 3324.671120\n",
      "Validation Loss: 3877.819336 \n",
      "\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "training loss: 3307.036355\n",
      "Validation Loss: 3927.688525 \n",
      "\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "training loss: 3288.699862\n",
      "Validation Loss: 3949.392676 \n",
      "\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "training loss: 3279.076349\n",
      "Validation Loss: 3965.927783 \n",
      "\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "training loss: 3257.013594\n",
      "Validation Loss: 3970.331104 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "training loss: 3244.500466\n",
      "Validation Loss: 3985.017285 \n",
      "\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "training loss: 3242.895419\n",
      "Validation Loss: 3898.901807 \n",
      "\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "training loss: 3223.303489\n",
      "Validation Loss: 3920.870313 \n",
      "\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "training loss: 3220.505482\n",
      "Validation Loss: 3931.131201 \n",
      "\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "training loss: 3211.737194\n",
      "Validation Loss: 3916.442041 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "training loss: 3206.760565\n",
      "Validation Loss: 3974.254932 \n",
      "\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "training loss: 3202.935303\n",
      "Validation Loss: 3896.046338 \n",
      "\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "training loss: 3193.535622\n",
      "Validation Loss: 3949.398828 \n",
      "\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "training loss: 3194.057440\n",
      "Validation Loss: 3938.304346 \n",
      "\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "training loss: 3190.742498\n",
      "Validation Loss: 3971.352588 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "training loss: 3185.159335\n",
      "Validation Loss: 3938.323779 \n",
      "\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "training loss: 3187.095304\n",
      "Validation Loss: 3892.902979 \n",
      "\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "training loss: 3186.947377\n",
      "Validation Loss: 3897.642822 \n",
      "\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "training loss: 3180.958052\n",
      "Validation Loss: 3981.211182 \n",
      "\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "training loss: 3178.911266\n",
      "Validation Loss: 3898.955469 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "training loss: 3183.362571\n",
      "Validation Loss: 4067.826514 \n",
      "\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "training loss: 3175.361373\n",
      "Validation Loss: 3968.776465 \n",
      "\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "training loss: 3181.354070\n",
      "Validation Loss: 4094.260059 \n",
      "\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "training loss: 3175.981157\n",
      "Validation Loss: 3900.147412 \n",
      "\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "training loss: 3172.404963\n",
      "Validation Loss: 4024.882959 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "training loss: 3177.387806\n",
      "Validation Loss: 3995.919971 \n",
      "\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "training loss: 3169.567272\n",
      "Validation Loss: 3943.016699 \n",
      "\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "training loss: 3172.683927\n",
      "Validation Loss: 3995.203760 \n",
      "\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "training loss: 3168.222412\n",
      "Validation Loss: 3991.955176 \n",
      "\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "training loss: 3167.124512\n",
      "Validation Loss: 4057.711230 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[89]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m lr = \u001b[32m0.00001\u001b[39m\n\u001b[32m      3\u001b[39m optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[87]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(train_loader, val_loader, model, optimizer)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m t % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m :\n\u001b[32m     55\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m-------------------------------\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m train_loss.append(loss)\n\u001b[32m     58\u001b[39m loss = validation(val_loader, model, loss_fn, t)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[87]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(dataloader, model, loss_fn, optimizer, epoch)\u001b[39m\n\u001b[32m     15\u001b[39m     loss.backward()\n\u001b[32m     16\u001b[39m     optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m epoch % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m :\n\u001b[32m     19\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtraining loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss/num_batches\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m>7f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = DnaCnn().to(device)\n",
    "lr = 0.00001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "train_model(train_loader, val_loader, model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd08fa91",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_count": 90,
   "id": "0c4c3967",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ea812b",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_count": 91,
   "id": "a8c705c4",
   "metadata": {},
   "outputs": [],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DnaCnn(\n",
       "  (conv_block): Sequential(\n",
       "    (0): Conv1d(4, 512, kernel_size=(1028,), stride=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0, inplace=False)\n",
       "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (regression_block): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=37, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(\"model.pth\", weights_only=False)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e70cbc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 37])\n"
     ]
    }
   ],
   "source": [
    "input, y = next(iter(train_loader))\n",
    "input = input.to(device)\n",
    "\n",
    "output = model.forward(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "99669921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tissue: Predicted, True\n",
      "0: 1217.1, 1217.8\n",
      "1: 852.3, 786.4\n",
      "2: 864.1, 856.1\n",
      "3: 944.2, 812.9\n",
      "4: 828.1, 910.6\n",
      "5: 903.6, 973.1\n",
      "6: 1.0, 684.8\n",
      "7: 786.9, 811.3\n",
      "8: 454.0, 0.0\n",
      "9: 891.8, 819.1\n",
      "10: 779.4, 818.5\n",
      "11: 907.4, 871.0\n",
      "12: 800.8, 494.1\n",
      "13: 707.8, 648.6\n",
      "14: 385.8, 0.0\n",
      "15: 867.6, 891.0\n",
      "16: 792.5, 737.0\n",
      "17: 1151.5, 1107.7\n",
      "18: 747.1, 826.8\n",
      "19: 613.8, 532.3\n",
      "20: 768.8, 596.3\n",
      "21: 1.0, 1016.2\n",
      "22: 1163.0, 1646.8\n",
      "23: 822.5, 654.2\n",
      "24: 1.0, 356.1\n",
      "25: 2030.9, 2050.0\n",
      "26: 869.6, 790.0\n",
      "27: 977.5, 1042.5\n",
      "28: 1065.3, 1217.7\n",
      "29: 1096.7, 1324.2\n",
      "30: 1104.5, 1189.1\n",
      "31: 846.7, 809.0\n",
      "32: 930.2, 858.1\n",
      "33: 1.0, 379.3\n",
      "34: 847.8, 787.6\n",
      "35: 1143.2, 1208.7\n",
      "36: 1.0, 362.0\n"
     ]
    }
   ],
   "source": [
    "i = 35\n",
    "print('Tissue: Predicted, True')\n",
    "for s, (y_p, y_t) in enumerate(zip(output[i], y[i])):\n",
    "    print(f'{s}: {torch.exp(y_p):.1f}, {y_t:.1f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e687ded3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
